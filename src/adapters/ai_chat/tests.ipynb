{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8135443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory (before): f:\\Dev\\interview-service\\interview-service\\src\\adapters\\ai_chat\n",
      "Project root: F:\\Dev\\interview-service\\interview-service\n",
      "Working directory (after): F:\\Dev\\interview-service\\interview-service\n",
      "pyproject.toml exists: True\n",
      "src/ exists: True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Get the current working directory (where notebook is executed from)\n",
    "current_dir = Path.cwd()\n",
    "\n",
    "# Start from current directory and search upward for project root\n",
    "# Project root should contain pyproject.toml (not in src/)\n",
    "project_root = current_dir\n",
    "max_levels = 10  # Safety limit\n",
    "\n",
    "for _ in range(max_levels):\n",
    "    # Check if this directory contains pyproject.toml\n",
    "    if (project_root / \"pyproject.toml\").exists():\n",
    "        # Verify it's the actual project root (not a subdirectory)\n",
    "        # Project root should have pyproject.toml and src/ directory\n",
    "        if (project_root / \"src\").exists() and (project_root / \"pyproject.toml\").exists():\n",
    "            break\n",
    "    if project_root == project_root.parent:\n",
    "        # Reached filesystem root\n",
    "        break\n",
    "    project_root = project_root.parent\n",
    "else:\n",
    "    # Fallback: go up 3 levels from current directory if we're in src/adapters/ai_chat/\n",
    "    if \"src\" in str(current_dir) and \"adapters\" in str(current_dir):\n",
    "        project_root = current_dir.parent.parent.parent\n",
    "\n",
    "# Add project root to Python path (must be absolute path)\n",
    "project_root = project_root.resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Change working directory to project root\n",
    "os.chdir(project_root)\n",
    "\n",
    "print(f\"Current directory (before): {current_dir}\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Working directory (after): {os.getcwd()}\")\n",
    "print(f\"pyproject.toml exists: {(project_root / 'pyproject.toml').exists()}\")\n",
    "print(f\"src/ exists: {(project_root / 'src').exists()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589c501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from src.adapters.ai_chat.ai_chat import AIChat\n",
    "from src.domain.vacancy.vacancy import VacancyInfo\n",
    "from src.domain.message.message import Message, RoleEnum, TypeEnum\n",
    "from src.domain.task.task import Task, TaskType\n",
    "import asyncio\n",
    "import dotenv   \n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# Вариант с доменом без порта (HTTPS):\n",
    "BASE_URL = \"https://llm.t1v.scibox.tech/v1\"\n",
    "# Альтернатива с IP:порт\n",
    "# BASE_URL = \"http://45.145.191.148:4000/v1\"\n",
    "\n",
    "client = OpenAI(api_key=API_KEY, base_url=BASE_URL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55277701",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# resp = client.chat.completions.create(\n",
    "#     model=\"qwen3-32b-awq\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"Ты дружелюбный помощник\"},\n",
    "#         {\"role\": \"user\", \"content\": \"Расскажи анекдот\"},\n",
    "#     ],\n",
    "#     temperature=0.7,\n",
    "#     top_p=0.9,    max_tokens=20000,\n",
    "# )\n",
    "\n",
    "# print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18e518d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with client.chat.completions.stream(\n",
    "#     model=\"qwen3-32b-awq\",\n",
    "#     messages=[{\"role\": \"user\", \"content\": \"Сделай краткое резюме книги Война и мир\"}],\n",
    "#     max_tokens=20000,\n",
    "# ) as stream:\n",
    "#     for event in stream:\n",
    "#         if event.type == \"chunk\":\n",
    "#             delta = getattr(event.chunk.choices[0].delta, \"content\", None)\n",
    "#             if delta:\n",
    "#                 print(delta, end=\"\", flush=True)\n",
    "#         elif event.type == \"message.completed\":\n",
    "#             print()  # newlinefrom openai import OpenAI\n",
    "# import dotenv   \n",
    "# import os\n",
    "\n",
    "# dotenv.load_dotenv()\n",
    "\n",
    "# API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# # Вариант с доменом без порта (HTTPS):\n",
    "# BASE_URL = \"https://llm.t1v.scibox.tech/v1\"\n",
    "# # Альтернатива с IP:порт\n",
    "# # BASE_URL = \"http://45.145.191.148:4000/v1\"\n",
    "\n",
    "# client = OpenAI(api_key=API_KEY, base_url=BASE_URL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf020b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ai_utils.misc import get_chat_completion_stream\n",
    "\n",
    "# stream = get_chat_completion_stream(client, \"qwen3-32b-awq\", [{\"role\": \"user\", \"content\": \"Сделай задачу для собеседования AI разработчика на Python и тесты для нее\"}], 20000)\n",
    "\n",
    "# for chunk in stream:\n",
    "#     print(chunk, end=\"\", flush=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ee1d46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinking...\n",
      "Finished thinking.\n",
      "\n",
      "Хорошо, начнём с задачи. Как вы планируете реализовать валидацию email? Опишите общий подход — например, будете ли использовать регулярные выражения, проверку наличия символа '@', домена и т.д.\n",
      "\n",
      "--- Full response ---\n",
      "Thinking...\n",
      "Finished thinking.\n",
      "\n",
      "Хорошо, начнём с задачи. Как вы планируете реализовать валидацию email? Опишите общий подход — например, будете ли использовать регулярные выражения, проверку наличия символа '@', домена и т.д.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from src.domain.vacancy.vacancy import VacancyInfo\n",
    "from src.domain.message.message import Message, RoleEnum, TypeEnum\n",
    "from src.domain.task.task import Task, TaskType\n",
    "\n",
    "# Create example vacancy info\n",
    "vacancy_info = VacancyInfo(\n",
    "    profession=\"Python разработчик\",\n",
    "    position=\"Senior Python Developer\",\n",
    "    requirements=\"Опыт работы с FastAPI, PostgreSQL, Docker\",\n",
    "    questions=\"Что такое декораторы в Python?\",\n",
    "    tasks=[\"Реализовать REST API для управления пользователями\"],\n",
    "    task_ides=[\"task_1\"]\n",
    ")\n",
    "\n",
    "# Create example chat history\n",
    "chat_history = [\n",
    "    Message(\n",
    "        role=RoleEnum.USER,\n",
    "        type=TypeEnum.QUESTION,\n",
    "        content=\"Привет! Можете рассказать о вакансии?\"\n",
    "    ),\n",
    "    Message(\n",
    "        role=RoleEnum.AI,\n",
    "        type=TypeEnum.ANSWER,\n",
    "        content=\"Конечно! Мы ищем Senior Python Developer с опытом работы с FastAPI.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create example task\n",
    "task = Task(\n",
    "    type=TaskType.CODE,\n",
    "    language=\"Python\",\n",
    "    description=\"Реализуйте функцию для валидации email адреса\"\n",
    ")\n",
    "\n",
    "# Use create_response (it's async and returns a generator)\n",
    "async def test_create_response():\n",
    "    ai_chat = AIChat()\n",
    "    response_chunks = []\n",
    "    \n",
    "    async for chunk in ai_chat.create_response(vacancy_info, chat_history, task):\n",
    "        response_chunks.append(chunk)\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "    \n",
    "    print(\"\\n\\n--- Full response ---\")\n",
    "    full_response = \"\".join(response_chunks)\n",
    "    print(full_response)\n",
    "\n",
    "# Run the async function\n",
    "await test_create_response()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bc096c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_chat = AIChat()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
