{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8135443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory (before): f:\\Dev\\interview-service\\interview-service\\src\\adapters\\ai_chat\n",
      "Project root: F:\\Dev\\interview-service\\interview-service\n",
      "Working directory (after): F:\\Dev\\interview-service\\interview-service\n",
      "pyproject.toml exists: True\n",
      "src/ exists: True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Get the current working directory (where notebook is executed from)\n",
    "current_dir = Path.cwd()\n",
    "\n",
    "# Start from current directory and search upward for project root\n",
    "# Project root should contain pyproject.toml (not in src/)\n",
    "project_root = current_dir\n",
    "max_levels = 10  # Safety limit\n",
    "\n",
    "for _ in range(max_levels):\n",
    "    \n",
    "    # Check if this directory contains pyproject.toml\n",
    "    if (project_root / \"pyproject.toml\").exists():\n",
    "        # Verify it's the actual project root (not a subdirectory)\n",
    "        # Project root should have pyproject.toml and src/ directory\n",
    "        if (project_root / \"src\").exists() and (project_root / \"pyproject.toml\").exists():\n",
    "            break\n",
    "    if project_root == project_root.parent:\n",
    "        # Reached filesystem root\n",
    "        break\n",
    "    project_root = project_root.parent\n",
    "else:\n",
    "    # Fallback: go up 3 levels from current directory if we're in src/adapters/ai_chat/\n",
    "    if \"src\" in str(current_dir) and \"adapters\" in str(current_dir):\n",
    "        project_root = current_dir.parent.parent.parent\n",
    "\n",
    "# Add project root to Python path (must be absolute path)\n",
    "project_root = project_root.resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Change working directory to project root\n",
    "os.chdir(project_root)\n",
    "\n",
    "print(f\"Current directory (before): {current_dir}\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Working directory (after): {os.getcwd()}\")\n",
    "print(f\"pyproject.toml exists: {(project_root / 'pyproject.toml').exists()}\")\n",
    "print(f\"src/ exists: {(project_root / 'src').exists()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589c501e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<openai.OpenAI object at 0x000001D50DD52A50>\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from src.adapters.ai_chat.ai_chat import AIChat\n",
    "from src.domain.vacancy.vacancy import VacancyInfo\n",
    "from src.domain.message.message import Message, RoleEnum, TypeEnum\n",
    "from src.domain.task.task import Task, TaskType\n",
    "import asyncio\n",
    "import dotenv   \n",
    "import os\n",
    "from config import MODEL_NAME, TOKEN_LIMIT\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# Вариант с доменом без порта (HTTPS):\n",
    "BASE_URL = \"https://llm.t1v.scibox.tech/v1\"\n",
    "# Альтернатива с IP:порт\n",
    "# BASE_URL = \"http://45.145.191.148:4000/v1\"\n",
    "\n",
    "client = OpenAI(api_key=API_KEY, base_url=BASE_URL)\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc9ab766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen3-32b-awq 25000\n"
     ]
    }
   ],
   "source": [
    "print(MODEL_NAME, TOKEN_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55277701",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# resp = client.chat.completions.create(\n",
    "#     model=\"qwen3-32b-awq\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"Ты дружелюбный помощник\"},\n",
    "#         {\"role\": \"user\", \"content\": \"Расскажи анекдот\"},\n",
    "#     ],\n",
    "#     temperature=0.7,\n",
    "#     top_p=0.9,    max_tokens=20000,\n",
    "# )\n",
    "\n",
    "# print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18e518d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with client.chat.completions.stream(\n",
    "#     model=\"qwen3-32b-awq\",\n",
    "#     messages=[{\"role\": \"user\", \"content\": \"Сделай краткое резюме книги Война и мир\"}],\n",
    "#     max_tokens=20000,\n",
    "# ) as stream:\n",
    "#     for event in stream:\n",
    "#         if event.type == \"chunk\":\n",
    "#             delta = getattr(event.chunk.choices[0].delta, \"content\", None)\n",
    "#             if delta:\n",
    "#                 print(delta, end=\"\", flush=True)\n",
    "#         elif event.type == \"message.completed\":\n",
    "#             print()  # newlinefrom openai import OpenAI\n",
    "# import dotenv   \n",
    "# import os\n",
    "\n",
    "# dotenv.load_dotenv()\n",
    "\n",
    "# API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# # Вариант с доменом без порта (HTTPS):\n",
    "# BASE_URL = \"https://llm.t1v.scibox.tech/v1\"\n",
    "# # Альтернатива с IP:порт\n",
    "# # BASE_URL = \"http://45.145.191.148:4000/v1\"\n",
    "\n",
    "# client = OpenAI(api_key=API_KEY, base_url=BASE_URL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf020b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ai_utils.misc import get_chat_completion_stream\n",
    "\n",
    "# stream = get_chat_completion_stream(client, \"qwen3-32b-awq\", [{\"role\": \"user\", \"content\": \"Сделай задачу для собеседования AI разработчика на Python и тесты для нее\"}], 20000)\n",
    "\n",
    "# for chunk in stream:\n",
    "#     print(chunk, end=\"\", flush=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "932b134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from src.domain.vacancy.vacancy import VacancyInfo\n",
    "from src.domain.message.message import Message, RoleEnum, TypeEnum\n",
    "from src.domain.task.task import Task, TaskType\n",
    "from datetime import timedelta\n",
    "\n",
    "# Create example vacancy info\n",
    "vacancy_info = VacancyInfo(\n",
    "    profession=\"Python разработчик / Data Scientist\",\n",
    "    position=\"Junior Python Developer\",\n",
    "    requirements=\"Pandas, Numpy, Tensorflow, PyTorch, SQLAlchemy\",\n",
    "    questions=\"\",\n",
    "    tasks=None,\n",
    "    task_ides=None,\n",
    "    interview_plan=\"\",  # Will be generated by create_chat\n",
    "    duration=timedelta(minutes=30)\n",
    ")\n",
    "\n",
    "# Create example chat history\n",
    "chat_history = [\n",
    "    # Message(\n",
    "    #     role=RoleEnum.USER,\n",
    "    #     type=TypeEnum.QUESTION,\n",
    "    #     content=\"Привет! Можете рассказать о вакансии?\"\n",
    "    # ),\n",
    "    # Message(\n",
    "    #     role=RoleEnum.AI,\n",
    "    #     type=TypeEnum.ANSWER,\n",
    "    #     content=\"Конечно! Мы ищем Senior Python Developer с опытом работы с FastAPI.\"\n",
    "    # )\n",
    "]\n",
    "\n",
    "# Create example task\n",
    "task = Task(\n",
    "    type=TaskType.CODE,\n",
    "    language=\"Python\",\n",
    "    description=\"Реализуйте функцию для валидации email адреса\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ee1d46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing create_chat...\n",
      "=== Updated Vacancy Info ===\n",
      "Profession: Python разработчик / Data Scientist\n",
      "Position: Junior Python Developer\n",
      "\n",
      "Interview Plan (first 300 chars):\n",
      "INTERNAL INTERVIEW PLAN ONLY (DO NOT SHARE WITH CANDIDATE)\n",
      "\n",
      "1. **Warm-up (5-10 min)**  \n",
      "   - [theory] Explain how to handle missing data in Pandas DataFrames. Demonstrate .dropna() vs .fillna() with example scenarios.  \n",
      "\n",
      "2. **Core Libraries Assessment (30-40 min)**  \n",
      "   - [coding] Numpy task: Write a function to normalize a 2D array (row-wise) and handle division by zero.  \n",
      "   - [theory] Compare Tensorflow and PyTorch: When would you choose one over the other? Discuss dynamic vs static computation graphs.  \n",
      "\n",
      "3. **ML Frameworks Practical (20-30 min)**  \n",
      "   - [coding] PyTorch task: Implement a single-layer neural network for binary classification (define model, loss function, and optimizer).  \n",
      "\n",
      "4. **Database Integration (15-20 min)**  \n",
      "   - [theory] Explain SQLAlchemy's ORM approach. How does it differ from raw SQL? Provide an example of a mapped class for a \"users\" table.  \n",
      "\n",
      "5. **Wrap-up (5 min)**  \n",
      "   - Open chat discussion: Ask candidate to explain their most complex Python project and challenges with data manipulation/ML pipeline.  \n",
      "\n",
      "Note: Prioritize coding tasks for Numpy/PyTorch, ensure theoretical questions validate understanding of core concepts. Adjust timing based on candidate pace....\n",
      "\n",
      "Full Interview Plan Length: 1209 characters\n",
      "\n",
      "==================================================\n",
      "\n",
      "Testing generate_welcome_message...\n",
      "\n",
      "=== Welcome Message (streaming) ===\n",
      "\n",
      "Thinking...\n",
      "Finished thinking.\n",
      "\n",
      "\n",
      "Здравствуйте! Добро пожаловать на техническое интервью.  \n",
      "В ходе интервью вы будете решать задачи на Python (Pandas, Numpy, PyTorch и др.) в встроенной среде. Время ограничено, поэтому работайте аккуратно и оперативно.  \n",
      "Вы можете задавать уточняющие вопросы по условию задач, но не ожидайте, что я напишу решение за вас — я помогу направить вас в правильное русло. Мы также обсудим ваш подход и код.  \n",
      "**Важно:** не используйте внешние инструменты (LLM, поисковики), не копируйте код извне — всё должно быть введено вручную. Попытки обойти правила приведут к дезавалидации интервью.  \n",
      "Чат будет проверен после завершения. Начнём с первой задачи — готовы?\n",
      "\n",
      "=== Full Welcome Message ===\n",
      "Thinking...\n",
      "Finished thinking.\n",
      "\n",
      "\n",
      "Здравствуйте! Добро пожаловать на техническое интервью.  \n",
      "В ходе интервью вы будете решать задачи на Python (Pandas, Numpy, PyTorch и др.) в встроенной среде. Время ограничено, поэтому работайте аккуратно и оперативно.  \n",
      "Вы можете задавать уточняющие вопросы по условию задач, но не ожидайте, что я напишу решение за вас — я помогу направить вас в правильное русло. Мы также обсудим ваш подход и код.  \n",
      "**Важно:** не используйте внешние инструменты (LLM, поисковики), не копируйте код извне — всё должно быть введено вручную. Попытки обойти правила приведут к дезавалидации интервью.  \n",
      "Чат будет проверен после завершения. Начнём с первой задачи — готовы?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test create_chat function\n",
    "async def test_create_chat():\n",
    "    ai_chat = AIChat()\n",
    "    \n",
    "    # create_chat returns updated VacancyInfo with interview_plan\n",
    "    updated_vacancy = await ai_chat.create_chat(vacancy_info, chat_history)\n",
    "    \n",
    "    print(\"=== Updated Vacancy Info ===\")\n",
    "    print(f\"Profession: {updated_vacancy.profession}\")\n",
    "    print(f\"Position: {updated_vacancy.position}\")\n",
    "    print(f\"\\nInterview Plan (first 300 chars):\")\n",
    "    print(updated_vacancy.interview_plan + \"...\" if len(updated_vacancy.interview_plan) > 300 else updated_vacancy.interview_plan)\n",
    "    print(f\"\\nFull Interview Plan Length: {len(updated_vacancy.interview_plan)} characters\")\n",
    "\n",
    "# Test generate_welcome_message function\n",
    "async def test_generate_welcome_message():\n",
    "    ai_chat = AIChat()\n",
    "    \n",
    "    # First create chat to get updated vacancy with interview_plan\n",
    "    updated_vacancy = await ai_chat.create_chat(vacancy_info, chat_history)\n",
    "    \n",
    "    print(\"\\n=== Welcome Message (streaming) ===\\n\")\n",
    "    welcome_chunks = []\n",
    "    \n",
    "    # ❌ before:\n",
    "    # async for chunk in ai_chat.generate_welcome_message(updated_vacancy, chat_history):\n",
    "\n",
    "    # ✅ after:\n",
    "    stream = await ai_chat.generate_welcome_message(updated_vacancy, chat_history)\n",
    "    async for chunk in stream:\n",
    "        welcome_chunks.append(chunk)\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "    \n",
    "    print(\"\\n\\n=== Full Welcome Message ===\")\n",
    "    full_welcome = \"\".join(welcome_chunks)\n",
    "    print(full_welcome)\n",
    "\n",
    "# Run the async functions\n",
    "print(\"Testing create_chat...\")\n",
    "await test_create_chat()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Testing generate_welcome_message...\")\n",
    "await test_generate_welcome_message()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a2453e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test create_response function (for reference)\n",
    "async def test_create_response():\n",
    "    ai_chat = AIChat()\n",
    "    \n",
    "    # First create chat to get updated vacancy\n",
    "    updated_vacancy = await ai_chat.create_chat(vacancy_info, chat_history)\n",
    "    \n",
    "    # Then create response with task\n",
    "    print(\"\\n=== Response to Task (streaming) ===\\n\")\n",
    "    response_chunks = []\n",
    "    stream = await ai_chat.create_response(updated_vacancy, chat_history, task)\n",
    "    async for chunk in stream:\n",
    "        response_chunks.append(chunk)\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "    \n",
    "    print(\"\\n\\n=== Full Response ===\")\n",
    "    full_response = \"\".join(response_chunks)\n",
    "    print(full_response)\n",
    "\n",
    "# Run the async function\n",
    "await test_create_response()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "651050e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_interview_plan = \"\"\"INTERNAL INTERVIEW PLAN ONLY - DO NOT SHARE WITH CANDIDATES  \n",
    "\n",
    "1. **Warm-up Question (5-7 min)**  \n",
    "   - [theory] *Explain the primary use cases for Pandas DataFrames vs. NumPy ndarrays. When would you choose one over the other?*  \n",
    "\n",
    "2. **Theoretical Questions (10-12 min)**  \n",
    "   - [theory] *What is the purpose of SQLAlchemy's ORM layer? How does it simplify database interactions compared to raw SQL?*  \n",
    "   - [theory] *Compare TensorFlow and PyTorch. In what scenarios is each framework typically preferred?*  \n",
    "\n",
    "3. **Core Coding Tasks (30-35 min)**  \n",
    "   - **Task 1** [coding] *Write Pandas code to load a CSV file, filter rows where column 'A' > 10, and calculate the mean of column 'B'.*  \n",
    "   - **Task 2** [coding] *Create a NumPy array of shape (3,3) filled with random values. Compute eigenvalues and perform matrix inversion.*  \n",
    "   - **Task 3** [coding] *Build a simple neural network (1 hidden layer) using PyTorch/TensorFlow to classify the Iris dataset (skeleton code provided). Compile and explain the model.*  \n",
    "\n",
    "4. **Follow-up & Debugging (10-12 min)**  \n",
    "   - [theory] *Explain how you would optimize the Pandas code for large datasets.*  \n",
    "   - [coding] *Debug a provided SQLAlchemy ORM query that fails to join two tables correctly.*  \n",
    "\n",
    "5. **Wrap-up (3-5 min)**  \n",
    "   - [theory] *What are the key challenges when integrating NumPy/TensorFlow for GPU-accelerated computations?*  \n",
    "\n",
    "---  \n",
    "**Timing Notes**: Adjust based on candidate performance. Prioritize depth in core libraries (Pandas, NumPy) over framework specifics.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dfee1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fee9357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTERNAL INTERVIEW PLAN ONLY - DO NOT SHARE WITH CANDIDATES  \n",
      "\n",
      "1. **Warm-up Question (5-7 min)**  \n",
      "   - [theory] *Explain the primary use cases for Pandas DataFrames vs. NumPy ndarrays. When would you choose one over the other?*  \n",
      "\n",
      "2. **Theoretical Questions (10-12 min)**  \n",
      "   - [theory] *What is the purpose of SQLAlchemy's ORM layer? How does it simplify database interactions compared to raw SQL?*  \n",
      "   - [theory] *Compare TensorFlow and PyTorch. In what scenarios is each framework typically preferred?*  \n",
      "\n",
      "3. **Core Coding Tasks (30-35 min)**  \n",
      "   - **Task 1** [coding] *Write Pandas code to load a CSV file, filter rows where column 'A' > 10, and calculate the mean of column 'B'.*  \n",
      "   - **Task 2** [coding] *Create a NumPy array of shape (3,3) filled with random values. Compute eigenvalues and perform matrix inversion.*  \n",
      "   - **Task 3** [coding] *Build a simple neural network (1 hidden layer) using PyTorch/TensorFlow to classify the Iris dataset (skeleton code provided). Compile and explain the model.*  \n",
      "\n",
      "4. **Follow-up & Debugging (10-12 min)**  \n",
      "   - [theory] *Explain how you would optimize the Pandas code for large datasets.*  \n",
      "   - [coding] *Debug a provided SQLAlchemy ORM query that fails to join two tables correctly.*  \n",
      "\n",
      "5. **Wrap-up (3-5 min)**  \n",
      "   - [theory] *What are the key challenges when integrating NumPy/TensorFlow for GPU-accelerated computations?*  \n",
      "\n",
      "---  \n",
      "**Timing Notes**: Adjust based on candidate performance. Prioritize depth in core libraries (Pandas, NumPy) over framework specifics.\n",
      "\n",
      "=== Streaming Task Description ===\n",
      "\n",
      "Thinking...\n",
      "Finished thinking.\n",
      "\n",
      "\n",
      "[theory]  \n",
      "Поясните основные случаи использования Pandas DataFrames и NumPy ndarrays. В каких ситуациях вы выберете один инструмент вместо другого?\n",
      "\n",
      "=== Full Task Description ===\n",
      "Thinking...\n",
      "Finished thinking.\n",
      "\n",
      "\n",
      "[theory]  \n",
      "Поясните основные случаи использования Pandas DataFrames и NumPy ndarrays. В каких ситуациях вы выберете один инструмент вместо другого?\n"
     ]
    }
   ],
   "source": [
    "# Test stream_task function\n",
    "async def test_stream_task():\n",
    "    ai_chat = AIChat()\n",
    "    \n",
    "    # First create chat to get updated vacancy with interview_plan\n",
    "    # updated_vacancy = await ai_chat.create_chat(vacancy_info, chat_history)\n",
    "    vacancy_info.interview_plan = test_interview_plan\n",
    "    updated_vacancy = vacancy_info\n",
    "    print(updated_vacancy.interview_plan)\n",
    "    # Then stream the task\n",
    "    print(\"\\n=== Streaming Task Description ===\\n\")\n",
    "    task_chunks = []\n",
    "    \n",
    "    stream = await ai_chat.stream_task(updated_vacancy, chat_history)\n",
    "    async for chunk in stream:\n",
    "        task_chunks.append(chunk)\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "    \n",
    "    print(\"\\n\\n=== Full Task Description ===\")\n",
    "    full_task = \"\".join(task_chunks)\n",
    "    print(full_task)\n",
    "\n",
    "# Run the async function\n",
    "await test_stream_task()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
